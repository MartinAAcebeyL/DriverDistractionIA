{"cells":[{"cell_type":"markdown","source":["\n"],"metadata":{"id":"Gn8XCY85djG9"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":39,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}}},"id":"d6UAZfyzRgA5","outputId":"c8963bb6-4774-47bd-ec04-a3e06d8fef17"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-b0068da6-633b-406f-968d-4c8e3007f897\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b0068da6-633b-406f-968d-4c8e3007f897\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}}],"source":["! pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle competitions download -c state-farm-distracted-driver-detection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eo-Lf_8RQLnY"},"outputs":[],"source":["# Importamos las librerias necesarias\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import os\n","import glob as gb\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random \n","from PIL import Image\n","from skimage import io\n","from skimage.transform import resize\n","from tqdm import tqdm\n","import pandas as pd \n","import albumentations as A\n","from scipy.io import loadmat\n","# from pathlib import Path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqBQhgKJ7Mgk"},"outputs":[],"source":["# Verificaremos si tenemos GPU\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZluoYtQ7RMct"},"outputs":[],"source":["# Descomprimiendo ek dataset\n","import zipfile\n","\n","with zipfile.ZipFile('/content/state-farm-distracted-driver-detection.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/conductores')"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"Lsny66qtdzDb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQwv8plw2Yh8"},"outputs":[],"source":["import os \n","\n","PATH = '/content/conductores/imgs/train'\n","\n","classes = os.listdir(PATH)\n","classes, len(classes) # 400 clases de aves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgPbcw9I3ZIo"},"outputs":[],"source":["imgs, labels = [], []     # Creamos el dataset de reatures y sus labels\n","\n","for i, lab in enumerate(classes):\n","  paths = os.listdir(f'{PATH}/{lab}')\n","  print(f'Categoría: {lab}. Imágenes: {len(paths)}')\n","  paths = [p for p in paths if p[-3:] == \"jpg\"]\n","  imgs += [f'{PATH}/{lab}/{img}' for img in paths]\n","  labels += [i]*len(paths)"]},{"cell_type":"code","source":["# Para verificar si todos tienen 3 canales\n","imgsN = []\n","labelsN = []\n","for i,x in enumerate(imgs):\n","  imgsN += [x]\n","  labelsN += [labels[i]]"],"metadata":{"id":"OpBPHNYaZy7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poONZfSG42ay"},"outputs":[],"source":["# plt.imshow(np.array(imgsN[1]))\n","img = plt.imread(imgsN[100])\n","plt.imshow(img), img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1X2dpJ86Pv_"},"outputs":[],"source":["# Longitud de las imgs de train\n","len(imgsN), len(labelsN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekS5ZVlh6WLf"},"outputs":[],"source":["# La clase dataset para preparar los datos\n","class Dataset(torch.utils.data.Dataset):\n","  def __init__(self, X, y, trans, device):\n","    self.X = X\n","    self.y = y\n","    self.trans = trans\n","    self.device = device\n","\n","  def __len__(self):\n","    return len(self.X)\n","\n","  def __getitem__(self, ix):\n","      img = io.imread(self.X[ix]) \n","      label = self.y[ix]\n","      img = self.trans(image=img)['image']         \n","      return torch.from_numpy(img / 255.).permute(2,0,1).float(), torch.tensor(label).long()"]},{"cell_type":"code","source":[""],"metadata":{"id":"thToJHwLeZM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKf8Tq7hYOQA"},"outputs":[],"source":["PATH = '/content/conductores/imgs/train'\n","\n","classesE = os.listdir(PATH)\n","classesE, len(classesE) # 400 clases de aves\n","\n","imgsE, labelsE = [], []     # Creamos el dataset de reatures y sus labels\n","\n","for i, lab in enumerate(classesE):\n","  paths = os.listdir(f'{PATH}/{lab}')\n","  print(f'Categoría: {lab}. Imágenes: {len(paths)}')\n","  paths = [p for p in paths if p[-3:] == \"jpg\"]\n","  imgsE += [f'{PATH}/{lab}/{img}' for img in paths]\n","  labelsE += [i]*len(paths)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bW4iiF2qZmvn"},"outputs":[],"source":["len(classesE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spajMgWQ6jPp"},"outputs":[],"source":["# Creramos el dataset para entrenar y evaluar\n","trans = A.Compose([\n","    A.Resize(224, 224)\n","])\n","\n","dataset = {\n","    'train': Dataset(imgsN, labelsN, trans, device), \n","    'test': Dataset(imgsE, labelsE, trans, device)\n","}\n","\n","len(dataset['train']), len(dataset['test'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKfzQ_kW7rGU"},"outputs":[],"source":["# Mostrando algunas imagenes del dataset\n","r, c = 1, 8\n","fig = plt.figure(figsize=(2*c, 2*r))\n","for _r in range(r):\n","    for _c in range(c):\n","        ax = plt.subplot(r, c, _r*c + _c + 1)\n","        ix = random.randint(0, len(dataset['train'])-1)\n","        img, label = dataset['train'][ix]\n","        plt.imshow(img.permute(1,2,0))\n","        plt.title(f'{classes[label][:6]}', color=\"black\")\n","        plt.axis(\"off\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QztKOxf99dn0"},"outputs":[],"source":["class Model(torch.nn.Module):\n","  def __init__(self, n_classes=len(classes), pretrained=False, freeze=False ):\n","    super().__init__()\n","    # descargamos resnet\n","    resnet = torchvision.models.resnet34(pretrained=pretrained)\n","    # nos quedamos con todas las capas menos la última\n","    self.resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n","    if freeze:\n","      for param in self.resnet.parameters():\n","        param.requires_grad=False\n","    # añadimos una nueva capa lineal para llevar a cabo la clasificación\n","    self.fc = torch.nn.Linear(512 , n_classes)\n","\n","  def forward(self, x):\n","    x = self.resnet(x)\n","    x = x.view(x.shape[0], -1)\n","    x = self.fc(x)\n","    return x\n","\n","  def unfreeze(self):\n","    for param in self.resnet.parameters():\n","        param.requires_grad=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnymnsYmCW-U"},"outputs":[],"source":["dataset['train'][100][0].shape  # Dimension de una imagen ya redimensinada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djSxzY1K-e7N"},"outputs":[],"source":["# Probamos las dimensiones y las dimensiones de salida de un batch al numero de clases a cladificar\n","model = Model()\n","outputs = model(torch.randn(64, 3, 224, 224))\n","outputs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6uKuTEFDoiv"},"outputs":[],"source":["dataloader = {\n","    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True, pin_memory=True), \n","    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False)\n","}\n","# Dimensiones de un batch de entrenamiento\n","imgs, labels = next(iter(dataloader['train']))\n","imgs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZNz1BUa_uTt"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","def fit(model, dataloader, epochs=5, lr=1e-2):\n","    model.to(device)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        train_loss, train_acc = [], []\n","        bar = tqdm(dataloader['train'])\n","        for batch in bar:\n","            X, y = batch\n","            X, y = X.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            y_hat = model(X)\n","            loss = criterion(y_hat, y)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss.append(loss.item())\n","            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n","            train_acc.append(acc)\n","            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n","        bar = tqdm(dataloader['test'])\n","        val_loss, val_acc = [], []\n","        model.eval()\n","        with torch.no_grad():\n","            for batch in bar:\n","                X, y = batch\n","                X, y = X.to(device), y.to(device)\n","                y_hat = model(X)\n","                loss = criterion(y_hat, y)\n","                val_loss.append(loss.item())\n","                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n","                val_acc.append(acc)\n","                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n","        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Anjbwh-bBZmt"},"outputs":[],"source":["# Fine Tunning\n","model = Model(pretrained=True, freeze=False)\n","fit(model, dataloader, epochs=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kz7r64YTg2ws"},"outputs":[],"source":["# # Fine Tunning segunda prueba 2\n","# model2 = Model(pretrained=True, freeze=False)\n","# fit(model2, dataloader, epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3UERpcFb6EO"},"outputs":[],"source":["# Evaluamos el modelo con los datos de evaluacion o test\n","model.to(device)\n","model.eval()\n","with torch.no_grad():\n","  bar = tqdm(dataloader['test'])\n","  test_acc = []\n","  for imgs, labs in bar:\n","    imgs, labs = imgs.to(device), labs.to(device)\n","    y_hat = model(imgs)\n","    acc = (torch.argmax(y_hat, axis=1) == labs).sum().item() / len(labs)\n","    test_acc.append(acc)\n","  print(f' acc {np.mean(test_acc):.5f}')"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"_3EJlJW-KIvX"}},{"cell_type":"code","source":[""],"metadata":{"id":"8lu0Rvv1KHbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dk_Ve4BiRhxo"},"outputs":[],"source":["# Prediciendo\n","r, c = 5, 10\n","fig = plt.figure(figsize=(2*c, 2*r))\n","checks = 0 \n","for _r in range(r):\n","    for _c in range(c):\n","        ax = plt.subplot(r, c, _r*c + _c + 1)\n","        ix = random.randint(0, len(dataset['test'])-1)\n","        img, label = dataset['test'][ix]\n","        y_hat = model(img.unsqueeze(0).to(device))\n","        lab = torch.argmax(y_hat, axis=1)[0].item()\n","        plt.imshow(img.permute(1,2,0))\n","        plt.title(f'{label}/{lab}', color=\"red\" if label != lab else \"green\"  )\n","        if label == lab:\n","          checks += 1\n","        plt.axis(\"off\")\n","plt.tight_layout()\n","plt.show()\n","print(checks)"]},{"cell_type":"markdown","source":["IOU-"],"metadata":{"id":"j8Ozz1V5Jc1X"}},{"cell_type":"code","source":["# 'c0': 'Safe driving', \n","#             'c1': 'Texting - right', \n","#             'c2': 'Talking on the phone - right', \n","#             'c3': 'Texting - left', \n","#             'c4': 'Talking on the phone - left', \n","#             'c5': 'Operating the radio', \n","#             'c6': 'Drinking', \n","#             'c7': 'Reaching behind', \n","#             'c8': 'Hair and makeup', \n","#             'c9': 'Talking to passenger'"],"metadata":{"id":"onFVfhrlKlow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model_1(torch.nn.Module):\n","  def __init__(self, n_classes=len(classes), pretrained=False, freeze=False ):\n","    super().__init__()\n","    # descargamos resnet\n","    resnet = torchvision.models.resnet50(pretrained=pretrained)\n","    # nos quedamos con todas las capas menos la última\n","    self.resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n","    if freeze:\n","      for param in self.resnet.parameters():\n","        param.requires_grad=False\n","    # añadimos una nueva capa lineal para llevar a cabo la clasificación\n","    self.fc = torch.nn.Linear(2048, n_classes)\n","\n","  def forward(self, x):\n","    x = self.resnet(x)\n","    x = x.view(x.shape[0], -1)\n","    x = self.fc(x)\n","    return x\n","\n","  def unfreeze(self):\n","    for param in self.resnet.parameters():\n","        param.requires_grad=True"],"metadata":{"id":"NcXk-ye7OMxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Probamos las dimensiones y las dimensiones de salida de un batch al numero de clases a cladificar\n","model = Model_1()\n","outputs = model(torch.randn(64, 3, 224, 224))\n","outputs.shape"],"metadata":{"id":"g6fPXBXoM2Sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","\n","def fit(model, dataloader, epochs=5, lr=1e-2):\n","    model.to(device)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        train_loss, train_acc = [], []\n","        bar = tqdm(dataloader['train'])\n","        for batch in bar:\n","            X, y = batch\n","            X, y = X.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            y_hat = model(X)\n","            loss = criterion(y_hat, y)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss.append(loss.item())\n","            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n","            train_acc.append(acc)\n","            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n","        bar = tqdm(dataloader['test'])\n","        val_loss, val_acc = [], []\n","        model.eval()\n","        with torch.no_grad():\n","            for batch in bar:\n","                X, y = batch\n","                X, y = X.to(device), y.to(device)\n","                y_hat = model(X)\n","                loss = criterion(y_hat, y)\n","                val_loss.append(loss.item())\n","                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n","                val_acc.append(acc)\n","                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n","        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"],"metadata":{"id":"7pp5ICy1NGeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fine Tunning\n","model = Model_1(pretrained=True, freeze=False)\n","fit(model, dataloader, epochs=5)"],"metadata":{"id":"zDrB-wSKNL4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluamos el modelo con los datos de evaluacion o test\n","model.to(device)\n","model.eval()\n","with torch.no_grad():\n","  bar = tqdm(dataloader['test'])\n","  test_acc = []\n","  for imgs, labs in bar:\n","    imgs, labs = imgs.to(device), labs.to(device)\n","    y_hat = model(imgs)\n","    acc = (torch.argmax(y_hat, axis=1) == labs).sum().item() / len(labs)\n","    test_acc.append(acc)\n","  print(f' acc {np.mean(test_acc):.5f}')"],"metadata":{"id":"ofQ9P-ZhbHQk"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"modelo_res18_.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}